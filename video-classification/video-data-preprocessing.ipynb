{"cells":[{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"joKTBjhfz-ZQ"}},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"buykpN7KbvvZ"}},{"cell_type":"code","source":["PHASE_CLASSES = 4\n","N_FRAMES = 10\n","BATCH_SIZE = 8"],"metadata":{"id":"Ef7IkCcfbx1h","executionInfo":{"status":"ok","timestamp":1692282220209,"user_tz":-60,"elapsed":547,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kF4uur5CqEL"},"source":["## Mount Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16788,"status":"ok","timestamp":1692282237500,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"},"user_tz":-60},"id":"dM3x5UhDh7Bv","outputId":"cdf2b1d7-b526-4011-94c3-ea62cb62fc9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1692282237501,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"},"user_tz":-60},"id":"sVvsrA9Hjl8z","outputId":"dfbed15b-7f3f-48bb-e2bd-b4b7bbee9378"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Individual Models FXAI\n"]}],"source":["%cd /content/drive/MyDrive/Individual Models FXAI/"]},{"cell_type":"markdown","source":["## Env setup"],"metadata":{"id":"JcGmPa6XU3Bi"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"SjI3AaaO16bd","executionInfo":{"status":"ok","timestamp":1692282304351,"user_tz":-60,"elapsed":66853,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"318265e9-ed93-4866-cc27-5648abddd572","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.5/558.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.2/365.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m669.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for farmhashpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.0 which is incompatible.\n","jsonschema 4.19.0 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n","pydantic 2.1.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.4.0 which is incompatible.\n","pydantic-core 2.4.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n","pymc 5.7.2 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n","referencing 0.30.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n","#!pip install -Uq \"tensorflow>=2.10.0\"\n","!pip install --quiet --upgrade tensorflow-federated"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"P5SBasQcbwQA","executionInfo":{"status":"ok","timestamp":1692282331535,"user_tz":-60,"elapsed":27188,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"fb20606a-f6b6-4030-8528-749ea5c3e04b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting remotezip\n","  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from remotezip) (2.31.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from remotezip) (0.9.0)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2023.7.22)\n","Building wheels for collected packages: remotezip\n","  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7933 sha256=643a3f16d712fbb3617d38af5093feb6853217c1b7f74a0a1ed5f0520108ac0a\n","  Stored in directory: /root/.cache/pip/wheels/fc/76/04/beed1a6df4eb7430ee13c3900746edd517e5e597298d1f73f3\n","Successfully built remotezip\n","Installing collected packages: remotezip\n","Successfully installed remotezip-0.12.1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dp-accounting 0.4.2 requires attrs~=21.4, but you have attrs 23.1.0 which is incompatible.\n","google-vizier 0.1.4 requires attrs==21.4.0, but you have attrs 23.1.0 which is incompatible.\n","tensorflow-federated 0.61.0 requires attrs~=21.4, but you have attrs 23.1.0 which is incompatible.\n","tensorflow-privacy 0.8.10 requires attrs~=21.4, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install remotezip tqdm opencv-python\n","!pip install -q git+https://github.com/tensorflow/docs"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9RYQIJ9C6BVH","executionInfo":{"status":"ok","timestamp":1692282336391,"user_tz":-60,"elapsed":4858,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"outputs":[],"source":["import tqdm\n","import random\n","import pathlib\n","import itertools\n","import collections\n","\n","import os\n","import cv2\n","import numpy as np\n","import remotezip as rz\n","\n","import tensorflow as tf\n","\n","# Some modules to display an animation using imageio.\n","import imageio\n","from IPython import display\n","from urllib import request\n","from tensorflow_docs.vis import embed\n","\n","# extras\n","from sklearn.model_selection import train_test_split\n","import keras\n","from keras.layers.convolutional import ( Conv2D, MaxPooling2D, AveragePooling2D)\n","from keras.layers import (    Input,    Activation,    Dense,    Flatten)\n","from keras.layers import add\n","from keras.layers import LayerNormalization\n","\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.utils import np_utils"]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"PD4PlQZFUyia"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"AgPUYwSvwY4g","executionInfo":{"status":"ok","timestamp":1692282337187,"user_tz":-60,"elapsed":799,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"outputs":[],"source":["NPZ_Name = 'Data/Videos_Database_20_Robot_WebCam_50_overall_database.npz'\n","Database_Used = np.load(NPZ_Name)\n","Sessions = Database_Used['Session']"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"h9gwArt8wY4i","executionInfo":{"status":"ok","timestamp":1692282337188,"user_tz":-60,"elapsed":7,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"outputs":[],"source":["# create a list of the unique sessions to become the client_ids\n","client_ids = np.unique(Sessions)"]},{"cell_type":"markdown","source":["### Get into format of classes and video files"],"metadata":{"id":"kBfw06OveVov"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"8AioEEtNwY4j","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1692282337188,"user_tz":-60,"elapsed":6,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"e53c4923-df53-44b6-9304-53a631e3f8f8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nThe dataset is a list of lists of lists\\nindex 0: class\\nindex 1: video selection\\nindex 2: frame selection\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["# Using code from tff to sort data\n","def create_fg_dataset(client_ids, Database_used_col, categorical, categories):\n","  dataset = [[] for _ in range(categories)]\n","  dataset_train = [[] for _ in range(categories)]\n","  dataset_test = [[] for _ in range(categories)]\n","  for session in client_ids:\n","    # find the indices of the current session in the Sessions column of Database_Used\n","    session_indices = np.where(Sessions == session)[0]\n","\n","    # get the X_train data for the current session\n","    session_X = Database_Used['X_train'][session_indices]\n","    # grab the training data for the necessary hierarchy\n","    session_Y = Database_Used[Database_used_col][session_indices]\n","    # if using categorical data, reshape the data for the model into one-hot encoded\n","    # if categorical==True:\n","    #   session_Y = np_utils.to_categorical(session_Y, categories)\n","\n","    '''\n","    Here is where we need to further split on classes\n","    '''\n","    # The session dataset will be a list of lists\n","    # The first index will represent the class\n","    # The second index will represent the frames in the session belonging to that class\n","\n","    # get the unique classes in the session\n","    session_classes = np.unique(session_Y)\n","\n","    # for each class in the session\n","    for session_class in session_classes:\n","        # get the indices of the current class\n","        class_indices = np.where(session_Y == session_class)[0]\n","        # get the X_train data for the current class\n","        class_X = session_X[class_indices]\n","        # append the class data to the dataset, using the class as the index\n","        dataset[session_class].append(class_X)\n","\n","  # create a train/test split of the dataset for each class\n","  for i in range(categories):\n","      dataset_train[i], dataset_test[i] = train_test_split(dataset[i], test_size=0.2)\n","\n","  return dataset_train, dataset_test\n","\n","'''\n","The dataset is a list of lists of lists\n","index 0: class\n","index 1: video selection\n","index 2: frame selection\n","'''"]},{"cell_type":"code","source":["X_train, X_test = create_fg_dataset(client_ids, 'Y_train_Context', True, PHASE_CLASSES)"],"metadata":{"id":"d5bVzpEGboPy","executionInfo":{"status":"ok","timestamp":1692282347490,"user_tz":-60,"elapsed":10307,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Shorten the video files appropriately"],"metadata":{"id":"PyOESNXYecpB"}},{"cell_type":"code","source":["def frames_from_video_file(video_frames, n_frames, frame_step = 15):\n","\n","  # Read each video frame by frame\n","  result = []\n","\n","  video_length = len(video_frames)\n","\n","  # Calculate the minimum required length for the video to get the desired\n","  # number of frames considering the frame step\n","  need_length = 1 + (n_frames - 1) * frame_step\n","\n","  # Either start at the beginning or at a random point between the beginning\n","  # and the latest possible point to ensure the desired number of frames\n","  # can be gathered\n","  if need_length > video_length:\n","    start = 0\n","  else:\n","    max_start = video_length - need_length\n","    start = random.randint(0, max_start + 1)\n","\n","  # Begin collecting the frames\n","  frame = video_frames[start]\n","  result.append(frame)\n","\n","  current = start\n","\n","  for _ in range(n_frames - 1):\n","    current += frame_step\n","    # If we have run out of space in the array, just fill it with black\n","    if current >= video_length:\n","      result.append(np.zeros_like(result[0]))\n","    else:\n","      frame = video_frames[current]\n","      result.append(frame)\n","\n","  return result"],"metadata":{"id":"NVUMFtpeef8G","executionInfo":{"status":"ok","timestamp":1692282347490,"user_tz":-60,"elapsed":17,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Need to get all the frames sorted pre-FrameGenerator\n","for i in range(len(X_train)):\n","  for j in range(len(X_train[i])):\n","    X_train[i][j] = frames_from_video_file(X_train[i][j], N_FRAMES)\n","    X_train[i][j] = np.array(X_train[i][j])\n","\n","for i in range(len(X_test)):\n","  for j in range(len(X_test[i])):\n","    X_test[i][j] = frames_from_video_file(X_test[i][j], N_FRAMES)\n","    X_test[i][j] = np.array(X_test[i][j])"],"metadata":{"id":"BG0ZbRqgmFvg","executionInfo":{"status":"ok","timestamp":1692282347490,"user_tz":-60,"elapsed":16,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["X_train[0][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imRyE5gSst9o","executionInfo":{"status":"ok","timestamp":1692282347491,"user_tz":-60,"elapsed":17,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"fba7edbf-d374-4d17-8b62-e610dfd0a5fb"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 50, 50, 3)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["class FrameGenerator:\n","  def __init__(self, dataset, training = False):\n","    \"\"\" Returns a set of frames with their associated label.\n","\n","      Args:\n","        path: Video file paths.\n","        n_frames: Number of frames.\n","        training: Boolean to determine if training dataset is being created.\n","    \"\"\"\n","    self.dataset = dataset\n","    self.training = training\n","    self.class_names = [i for i in range(len(dataset))]\n","\n","  def get_videos_and_labels(self):\n","    videos = []\n","    labels = []\n","    for i in range(len(self.dataset)):\n","      for j in range(len(self.dataset[i])):\n","        videos.append(self.dataset[i][j])\n","        labels.append(i)\n","    return videos, labels\n","\n","  def __call__(self):\n","    # No longer using the training boolean to shuffle data, not an issue just\n","    # something to be aware of\n","\n","    videos, labels = self.get_videos_and_labels()\n","\n","    pairs = list(zip(videos, labels))\n","\n","    if self.training:\n","      random.shuffle(pairs)\n","\n","    for video_frames, label in pairs:\n","      yield video_frames, label"],"metadata":{"id":"IQbHP7zVhFyj","executionInfo":{"status":"ok","timestamp":1692282347491,"user_tz":-60,"elapsed":16,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Test out the FrameGenerator\n","fg = FrameGenerator(X_train)\n","\n","frames, label = next(fg())\n","\n","print(f\"Shape: {frames.shape}\")\n","print(f\"Label: {label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snR9IANcnSus","executionInfo":{"status":"ok","timestamp":1692282347491,"user_tz":-60,"elapsed":16,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"27e770fe-4d8b-4b61-cce2-e6d00104f06a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape: (10, 50, 50, 3)\n","Label: 0\n"]}]},{"cell_type":"code","source":["# Create the training set\n","output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n","                    tf.TensorSpec(shape = (), dtype = tf.int16))\n","train_ds = tf.data.Dataset.from_generator(FrameGenerator(X_train, training=True),\n","                                          output_signature = output_signature)"],"metadata":{"id":"2LZUEovQwvSJ","executionInfo":{"status":"ok","timestamp":1692282352081,"user_tz":-60,"elapsed":4604,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Making sure they are shuffled\n","for frames, labels in train_ds.take(10):\n","  print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mcC94riyhr3","executionInfo":{"status":"ok","timestamp":1692282352081,"user_tz":-60,"elapsed":13,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"5ebbd5cc-06e7-420a-e27d-9d12010bf3d9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(2, shape=(), dtype=int16)\n","tf.Tensor(3, shape=(), dtype=int16)\n","tf.Tensor(3, shape=(), dtype=int16)\n","tf.Tensor(0, shape=(), dtype=int16)\n","tf.Tensor(3, shape=(), dtype=int16)\n","tf.Tensor(1, shape=(), dtype=int16)\n","tf.Tensor(0, shape=(), dtype=int16)\n","tf.Tensor(3, shape=(), dtype=int16)\n","tf.Tensor(2, shape=(), dtype=int16)\n","tf.Tensor(1, shape=(), dtype=int16)\n"]}]},{"cell_type":"code","source":["# Create test set\n","test_ds = tf.data.Dataset.from_generator(FrameGenerator(X_test, training=True),\n","                                          output_signature = output_signature)"],"metadata":{"id":"4p--VIi4yqRI","executionInfo":{"status":"ok","timestamp":1692282352081,"user_tz":-60,"elapsed":11,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Print the shapes of the data\n","train_frames, train_labels = next(iter(train_ds))\n","print(f'Shape of training set of frames: {train_frames.shape}')\n","print(f'Shape of training labels: {train_labels.shape}')\n","\n","test_frames, test_labels = next(iter(test_ds))\n","print(f'Shape of validation set of frames: {test_frames.shape}')\n","print(f'Shape of validation labels: {test_labels.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kZSs0vFyzFv","executionInfo":{"status":"ok","timestamp":1692282352574,"user_tz":-60,"elapsed":504,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"68470031-ee02-4aec-b145-77b5ee275b27"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training set of frames: (10, 50, 50, 3)\n","Shape of training labels: ()\n","Shape of validation set of frames: (10, 50, 50, 3)\n","Shape of validation labels: ()\n"]}]},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n","test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"],"metadata":{"id":"qNjZViLL3kRT","executionInfo":{"status":"ok","timestamp":1692282352575,"user_tz":-60,"elapsed":5,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["train_ds = train_ds.batch(BATCH_SIZE)\n","test_ds = test_ds.batch(BATCH_SIZE)\n","\n","train_frames, train_labels = next(iter(train_ds))\n","print(f'Shape of training set of frames: {train_frames.shape}')\n","print(f'Shape of training labels: {train_labels.shape}')\n","\n","test_frames, test_labels = next(iter(test_ds))\n","print(f'Shape of validation set of frames: {test_frames.shape}')\n","print(f'Shape of validation labels: {test_labels.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"amOgExT43n1l","executionInfo":{"status":"ok","timestamp":1692282352575,"user_tz":-60,"elapsed":4,"user":{"displayName":"Harrison Field","userId":"02417387657116967688"}},"outputId":"3fc2fc1d-eacc-403b-b26d-9617f82e32a2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training set of frames: (8, 10, 50, 50, 3)\n","Shape of training labels: (8,)\n","Shape of validation set of frames: (8, 10, 50, 50, 3)\n","Shape of validation labels: (8,)\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/video.ipynb","timestamp":1692269290649}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}