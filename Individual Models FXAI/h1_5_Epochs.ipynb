{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApTLjs2YEioT"
   },
   "source": [
    "Much of the code used to implement federated learning is taken or adapted from https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kF4uur5CqEL"
   },
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18041,
     "status": "ok",
     "timestamp": 1695201919464,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "dM3x5UhDh7Bv",
    "outputId": "5122ef9a-5007-4e19-d60d-e70c3d8404fe"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1695201919465,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "sVvsrA9Hjl8z",
    "outputId": "9a070ed3-1187-431e-f155-1c7e8b45f1ff"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/Individual Models FXAI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4mBmgkcClWX"
   },
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46093,
     "status": "ok",
     "timestamp": 1695201965555,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "kyYHWVUwq84L",
    "outputId": "bcf27154-891a-4b72-9f45-8e42ae0a5dab"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade tensorflow-federated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AxTKUDHwY4G"
   },
   "source": [
    "# Training H1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FV6zs1AwY4Q"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4829,
     "status": "ok",
     "timestamp": 1695201970380,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "yq61Zr1jwY4S"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "import collections\n",
    "from tensorflow.keras.models import load_model  # Use tensorflow.keras consistently\n",
    "\n",
    "# From EHIL\n",
    "\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, AveragePooling2D)\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten)\n",
    "from tensorflow.keras.layers import add, LayerNormalization\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.utils import to_categorical  # Replace np_utils\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201970380,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "qhfYnM2tKVny",
    "outputId": "46b4d888-9b6e-4904-c556-db444dff775e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w40GDwGfwY4c"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695201970381,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "7w46DSatwY4d"
   },
   "outputs": [],
   "source": [
    "img_channels = 3\n",
    "BATCH_SIZE = 20\n",
    "NUM_CLIENTS = 25\n",
    "PHASE_CLASSES = 4\n",
    "EPOCHS = 400\n",
    "LOGS_DIR = 'H1'\n",
    "MODEL_NAME = 'h1_25_clients_400_rounds_2_epochs'\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "img_rows, img_cols = 256,256\n",
    "img_rows, img_cols = 50,50\n",
    "\n",
    "Cup_Type = 'Big'\n",
    "if Cup_Type == 'Medium':\n",
    "    nb_classes = 9\n",
    "if Cup_Type == 'Big':\n",
    "    nb_classes = 10\n",
    "if Cup_Type == 'Small':\n",
    "    nb_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JuPVceJJAfE"
   },
   "source": [
    "We set up the summary writer to log our model's performance through training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1695201970811,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "J7pAhQrxwY4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 14:13:36.021213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-30 14:13:36.064388: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "summary_writer_h1 = tf.summary.create_file_writer('logs/' + LOGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtyIuN51wY4f"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1695201971055,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "AgPUYwSvwY4g"
   },
   "outputs": [],
   "source": [
    "NPZ_Name = 'Data/Videos_Database_20_Robot_WebCam_50_overall_database.npz'\n",
    "Database_Used = np.load(NPZ_Name)\n",
    "Sessions = Database_Used['Session']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201971055,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "h9gwArt8wY4i"
   },
   "outputs": [],
   "source": [
    "# create a list of the unique sessions to become the client_ids\n",
    "client_ids = np.unique(Sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201971056,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "8AioEEtNwY4j"
   },
   "outputs": [],
   "source": [
    "# Several steps need to be completed to convert our data to a format suitable for tensorflow-federated operations\n",
    "# The first is to create tf.data.Dataset objects\n",
    "def create_tf_dataset(client_ids, Database_used_col, categorical, categories):\n",
    "  train_datasets = []\n",
    "  test_datasets = []\n",
    "  for session in client_ids:\n",
    "    # find the indices of the current session in the Sessions column of Database_Used\n",
    "    session_indices = np.where(Sessions == session)[0]\n",
    "\n",
    "    # get the X_train data for the current session\n",
    "    session_X = Database_Used['X_train'][session_indices]\n",
    "    # grab the training data for the necessary hierarchy\n",
    "    session_Y = Database_Used[Database_used_col][session_indices]\n",
    "    # if using categorical data, reshape the data for the model into one-hot encoded\n",
    "    if categorical==True:\n",
    "      session_Y = np_utils.to_categorical(session_Y, categories)\n",
    "    # create train/test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(session_X, session_Y, test_size=0.2, random_state=100)\n",
    "    # Make into tf dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train,Y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "    # Add to our list of datasets\n",
    "    train_datasets.append([session,train_dataset])\n",
    "    test_datasets.append([session,test_dataset])\n",
    "  return train_datasets, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6784,
     "status": "ok",
     "timestamp": 1695201977836,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "30LYEUZMwY4k"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m phase_train_datasets, phase_test_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY_train_Context\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPHASE_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mcreate_tf_dataset\u001b[0;34m(client_ids, Database_used_col, categorical, categories)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# if using categorical data, reshape the data for the model into one-hot encoded\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categorical\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m   session_Y \u001b[38;5;241m=\u001b[39m \u001b[43mnp_utils\u001b[49m\u001b[38;5;241m.\u001b[39mto_categorical(session_Y, categories)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# create train/test split\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(session_X, session_Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np_utils' is not defined"
     ]
    }
   ],
   "source": [
    "phase_train_datasets, phase_test_datasets = create_tf_dataset(client_ids, 'Y_train_Context', True, PHASE_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1695201977836,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "ieLkpiiewY4l"
   },
   "outputs": [],
   "source": [
    "def make_client_data(datasets):\n",
    "  client_data = {}\n",
    "  # loop through the datasets\n",
    "  for dataset in datasets:\n",
    "      # get the session name\n",
    "      session = dataset[0]\n",
    "\n",
    "      # get the session data\n",
    "      session_data = dataset[1]\n",
    "\n",
    "      # add the session data to the client_data dictionary\n",
    "      client_data[session] = session_data\n",
    "  return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695201977836,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "SlDokkqmwY4n"
   },
   "outputs": [],
   "source": [
    "# Make everything into a map for creating ClientData objects necessary for TF federated learning\n",
    "\n",
    "phase_train_client_data = make_client_data(phase_train_datasets)\n",
    "phase_test_client_data = make_client_data(phase_test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201977836,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "77M86lhcwY4o"
   },
   "outputs": [],
   "source": [
    "client_ids = list(client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695201977837,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "YSPeJ7iq_z2b",
    "outputId": "4c97ba4f-befa-4ab3-f06e-95566b391c4d"
   },
   "outputs": [],
   "source": [
    "len(client_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiH95XycwY4p"
   },
   "source": [
    "## Setup Federated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695201977837,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "l9NqvebNwY4p"
   },
   "outputs": [],
   "source": [
    "def make_federated_data(client_data, client_ids):\n",
    "  # Need a function to get the client data in order to make ClientData object\n",
    "  def get_client_dataset(client_id):\n",
    "    return client_data[client_id]\n",
    "\n",
    "  # use tff to create ClientData object from our training data\n",
    "  federated_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(client_ids, get_client_dataset)\n",
    "\n",
    "  return federated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201977838,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "-iPrgbpYwY4p"
   },
   "outputs": [],
   "source": [
    "phase_train_federated_data = make_federated_data(phase_train_client_data, client_ids)\n",
    "phase_test_federated_data = make_federated_data(phase_test_client_data, client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201977838,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "WMU57P7ZwY4q"
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset, num_classes):\n",
    "    def batch_format_fn(image, label):\n",
    "        \"\"\"Prepare a batch of data and return a (features, label) tuple.\"\"\"\n",
    "        batch_size = tf.shape(image)[0]  # Get the current batch size\n",
    "        return (tf.reshape(image, [batch_size, 50, 50, 3]),\n",
    "                tf.reshape(label, [batch_size, num_classes]))\n",
    "\n",
    "    return dataset.batch(BATCH_SIZE).map(batch_format_fn)\n",
    "\n",
    "def preprocess_federated_data(federated_data, num_classes):\n",
    "  client_ids = sorted(federated_data.client_ids)[:NUM_CLIENTS]\n",
    "  print(client_ids)\n",
    "  federated_data = [preprocess(federated_data.create_tf_dataset_for_client(x), num_classes)\n",
    "                          for x in client_ids]\n",
    "  return federated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1695201978288,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "8t3Jm81MwY4q",
    "outputId": "844cd038-b05d-4a5f-dca8-dca4ca504b2e"
   },
   "outputs": [],
   "source": [
    "phase_train = preprocess_federated_data(phase_train_federated_data, PHASE_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695201978288,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "9tFAZcM2wY4r",
    "outputId": "01d5d7ae-ba05-42fa-c6b0-4fc8b3d08af8"
   },
   "outputs": [],
   "source": [
    "phase_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqmhVaMvwY4r"
   },
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2QEnw6DwY4s"
   },
   "source": [
    "### Functions from Dandan's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1695201978288,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "O4f6H9W3wY4s"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "'''\n",
    "Functions\n",
    "'''\n",
    "###############################################################################\n",
    "\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    '''\n",
    "    epoch: number of epochs for model training\n",
    "    lr: learning rate\n",
    "    '''\n",
    "    lr = 1e-3\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 40:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "def Conv_bn_relu(infor, **conv_params):\n",
    "    '''\n",
    "    Build conv -> BN -> relu block\n",
    "    '''\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                  strides=strides, padding=padding,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  kernel_regularizer=kernel_regularizer)(infor)\n",
    "\n",
    "\n",
    "    norm = LayerNormalization(axis=CHANNEL_AXIS)(conv)\n",
    "    out = Activation(\"relu\")(norm)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "#Reference: http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "def Bn_relu_conv(infor,**conv_params):\n",
    "    '''\n",
    "    Build a BN -> relu -> conv block.\n",
    "    '''\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    norm = LayerNormalization(axis=CHANNEL_AXIS)(infor)\n",
    "\n",
    "    activation = Activation(\"relu\")(norm)\n",
    "\n",
    "    out = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                      strides=strides, padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def basic_block(BlockIn, filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    '''\n",
    "    Basic 3 X 3 convolution blocks\n",
    "    '''\n",
    "\n",
    "    if is_first_block_of_first_layer:\n",
    "        conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                       strides=init_strides,\n",
    "                       padding=\"same\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2(1e-4))(BlockIn)\n",
    "    else:\n",
    "        conv1 = Bn_relu_conv(infor = BlockIn,filters=filters, kernel_size=(3, 3),\n",
    "                              strides=init_strides)\n",
    "\n",
    "    residual = Bn_relu_conv(infor = conv1,filters=filters, kernel_size=(3, 3))\n",
    "\n",
    "\n",
    "    input_shape = K.int_shape(BlockIn)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "\n",
    "    # stride should be set properly and match  (width, height) of residual\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "\n",
    "\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(BlockIn)\n",
    "    else:\n",
    "        shortcut = BlockIn\n",
    "\n",
    "    # Adds a shortcut between input and residual block\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def buildmodel(input_shape, num_outputs, Regress_Flag):\n",
    "        '''\n",
    "\n",
    "        input_shape: (nb_channels, nb_rows, nb_cols)\n",
    "        num_outputs:  number of outputs at final softmax layer\n",
    "        Regress_Flag: classify or regress\n",
    "\n",
    "        '''\n",
    "\n",
    "        global ROW_AXIS\n",
    "        global COL_AXIS\n",
    "        global CHANNEL_AXIS\n",
    "\n",
    "        # if K.image_data_format() == 'channels_last':\n",
    "\n",
    "        #     ROW_AXIS = 1; COL_AXIS = 2;  CHANNEL_AXIS = 3\n",
    "        # else:\n",
    "        #     CHANNEL_AXIS = 1; ROW_AXIS = 2; COL_AXIS = 3\n",
    "\n",
    "        if tf.keras.backend.image_data_format() == 'channels_last':\n",
    "            ROW_AXIS = 1\n",
    "            COL_AXIS = 2\n",
    "            CHANNEL_AXIS = 3\n",
    "        else:\n",
    "            CHANNEL_AXIS = 1\n",
    "            ROW_AXIS = 2\n",
    "            COL_AXIS = 3\n",
    "\n",
    "\n",
    "\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "        #if K.image_dim_ordering() == 'tf':\n",
    "\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "\n",
    "        input = Input(shape=input_shape, name='main_input')\n",
    "        conv1 = Conv_bn_relu(input, filters=64, kernel_size=(7, 7), strides=(2, 2))\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\", name = \"conv_pool1\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate([2,2,2,2]):\n",
    "            '''\n",
    "            residual block with repeating bottleneck blocks\n",
    "            '''\n",
    "\n",
    "            is_first_layer=(i == 0)\n",
    "            for i in range(r):\n",
    "                init_strides = (1, 1)\n",
    "                if i == 0 and not is_first_layer:\n",
    "                    init_strides = (2, 2)\n",
    "                block = basic_block(BlockIn = block, filters=filters, init_strides=init_strides,\n",
    "                                       is_first_block_of_first_layer=(is_first_layer and i == 0))\n",
    "\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "\n",
    "        norm = LayerNormalization(axis=CHANNEL_AXIS)(block)\n",
    "        block = Activation(\"relu\")(norm)\n",
    "\n",
    "        # classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2_out = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "                                 strides=(1, 1), name = \"conv_final\")(block)\n",
    "\n",
    "        #flatten1 = keras.layers.GlobalAveragePooling2D(name = \"GAP\")(pool2)\n",
    "        #out = keras.layers.Dense(num_outputs,activation='softmax')(pooled)\n",
    "\n",
    "        flatten1 = Flatten( name = \"Flatten\")(pool2_out)\n",
    "\n",
    "        flatten1 = Dense(128, activation='relu',name = \"Flatten2\")(flatten1)\n",
    "\n",
    "        if Regress_Flag == False:\n",
    "\n",
    "            dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
    "                      activation=\"softmax\", name = \"Dense_layer\")(flatten1)\n",
    "        else:\n",
    "            dense = Dense(units=1, activation='linear', kernel_initializer=glorot_uniform(seed=0))(flatten1)\n",
    "\n",
    "            #dense = Dense(units=1, kernel_initializer=\"he_normal\", activation=\"linear\", name = \"Dense_layer\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "\n",
    "def learn_model(oldmodel,nb_classes,Transfer_Type,summary=False):\n",
    "\n",
    "    base_model = oldmodel\n",
    "\n",
    "    if Transfer_Type == 'Classification':\n",
    "        intermediate_layer_model = Model(inputs=base_model.input,outputs=base_model.get_layer(\"conv_final\").output)\n",
    "\n",
    "    if Transfer_Type == 'Regression':\n",
    "        #intermediate_layer_model = Model(inputs=base_model.input,outputs=base_model.get_layer(\"Flatten1\").output)\n",
    "        intermediate_layer_model = Model(inputs=base_model.input,outputs=base_model.get_layer(\"Dense_Classification\").output)\n",
    "\n",
    "    x = intermediate_layer_model(base_model.input)\n",
    "    if nb_classes >1:\n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)# 添加全局平均池化层\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "    if Transfer_Type == 'Classification':\n",
    "        dense = Dense(units=nb_classes, kernel_initializer=\"he_normal\",\n",
    "                  activation=\"softmax\", name = \"Dense_Classification\")(x)\n",
    "    if Transfer_Type == 'Regression':\n",
    "        dense = Dense(units=1, activation='linear', name = \"Dense_Regression\", kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=dense)\n",
    "\n",
    "\n",
    "    # show summary if specified\n",
    "    if summary==True :\n",
    "        model.summary()\n",
    "\n",
    "    if Transfer_Type == 'Classification':\n",
    "        # choose the optimizer\n",
    "        #optimizer = keras.optimizers.Adam()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Transfer_Type == 'Regression':\n",
    "        model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics=['mse'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFwI5Z8owY4t"
   },
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1695201978288,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "afbxMkmtwY4t"
   },
   "outputs": [],
   "source": [
    "# It makes the implementation simpler to remove any arguments from the model building function\n",
    "def build_h1_model():\n",
    "  return buildmodel((img_channels, img_rows, img_cols), PHASE_CLASSES,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1695201979060,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "FOl30InqqPsS"
   },
   "outputs": [],
   "source": [
    "# View the model architecture\n",
    "model = build_h1_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2251,
     "status": "ok",
     "timestamp": 1695201981308,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "3iHMVK0PgsBW",
    "outputId": "cb47b91b-34b2-4352-f1ed-fb24dd220efa"
   },
   "outputs": [],
   "source": [
    "# View the model architecture\n",
    "model = build_h1_model()\n",
    "keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1695201981309,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "X4Sjp_5ehAN_",
    "outputId": "849dd7fc-540d-457b-99b5-13f9788793d1"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP1avAWNwY4u"
   },
   "source": [
    "## Setup Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201981309,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "7MatB9ShwY4u"
   },
   "outputs": [],
   "source": [
    "def model_fn_h1():\n",
    "  model = build_h1_model()\n",
    "  return tff.learning.models.from_keras_model(\n",
    "      model,\n",
    "      input_spec=phase_train[0].element_spec,\n",
    "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1695201981950,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "kfb47mqnwY4v"
   },
   "outputs": [],
   "source": [
    "@tff.tf_computation\n",
    "def server_init_h1():\n",
    "  model = model_fn_h1()\n",
    "  return model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201981951,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "ez4Z8rBHwY4v"
   },
   "outputs": [],
   "source": [
    "@tff.federated_computation\n",
    "def initialize_fn_h1():\n",
    "  return tff.federated_value(server_init_h1(), tff.SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201981951,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "YzPe2I_bwY4v"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def client_update_h1(model, dataset, server_weights, client_optimizer):\n",
    "  \"\"\"Performs training (using the server model weights) on the client's dataset.\"\"\"\n",
    "  # Initialize the client model with the current server weights.\n",
    "  client_weights = model.trainable_variables\n",
    "  # Assign the server weights to the client model.\n",
    "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                        client_weights, server_weights)\n",
    "\n",
    "\n",
    "  for i in range(2):\n",
    "  # Use the client_optimizer to update the local model.\n",
    "    for batch in dataset:\n",
    "      with tf.GradientTape() as tape:\n",
    "        # Compute a forward pass on the batch of data\n",
    "        outputs = model.forward_pass(batch)\n",
    "\n",
    "      # Compute the corresponding gradient\n",
    "      grads = tape.gradient(outputs.loss, client_weights)\n",
    "      grads_and_vars = zip(grads, client_weights)\n",
    "\n",
    "      # Apply the gradient using a client optimizer.\n",
    "      client_optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "  return client_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201981951,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "KgmhltlKwY4w"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def server_update_h1(model, mean_client_weights):\n",
    "  \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "  model_weights = model.trainable_variables\n",
    "  # Assign the mean client weights to the server model.\n",
    "  tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                        model_weights, mean_client_weights)\n",
    "  return model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1695201983034,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "CQ_vy5RGwY4w"
   },
   "outputs": [],
   "source": [
    "h1_fed = model_fn_h1()\n",
    "tf_dataset_type_h1 = tff.SequenceType(h1_fed.input_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1695201983034,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "q7ys4Z6OwY4w",
    "outputId": "d2aeb25c-3a57-4ca2-ad71-f51c4a732fe4"
   },
   "outputs": [],
   "source": [
    "str(tf_dataset_type_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695201983034,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "2GOtksdNwY4x"
   },
   "outputs": [],
   "source": [
    "model_weights_type_h1 = h1_fed.trainable_variables\n",
    "# Assuming model_weights_type is a list of trainable variables\n",
    "model_weights_type_h1 = [v for v in model_weights_type_h1]\n",
    "\n",
    "model_weights_type_h1 = tff.to_type([tf.TensorSpec.from_tensor(v.value()) for v in model_weights_type_h1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 5671,
     "status": "ok",
     "timestamp": 1695201988699,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "Ofu8zg0jwY4x"
   },
   "outputs": [],
   "source": [
    "@tff.tf_computation(tf_dataset_type_h1, model_weights_type_h1)\n",
    "def client_update_fn_h1(tf_dataset, server_weights):\n",
    "  model = model_fn_h1()\n",
    "  client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "  return client_update_h1(model, tf_dataset, server_weights, client_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1695201990241,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "lCEAv4MMwY4y"
   },
   "outputs": [],
   "source": [
    "@tff.tf_computation(model_weights_type_h1)\n",
    "def server_update_fn_h1(mean_client_weights):\n",
    "  model = model_fn_h1()\n",
    "  return server_update_h1(model, mean_client_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1695201990241,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "AS4EpHW4wY4y"
   },
   "outputs": [],
   "source": [
    "federated_server_type_h1 = tff.FederatedType(model_weights_type_h1, tff.SERVER)\n",
    "federated_dataset_type_h1 = tff.FederatedType(tf_dataset_type_h1, tff.CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201990241,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "sxMe8Eg0wY49"
   },
   "outputs": [],
   "source": [
    "@tff.federated_computation(federated_server_type_h1, federated_dataset_type_h1)\n",
    "def next_fn_h1(server_weights, federated_dataset):\n",
    "  # Broadcast the server weights to the clients.\n",
    "  server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "  # Each client computes their updated weights.\n",
    "  client_weights = tff.federated_map(\n",
    "      client_update_fn_h1, (federated_dataset, server_weights_at_client))\n",
    "\n",
    "  # The server averages these updates.\n",
    "  mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "  # The server updates its model.\n",
    "  server_weights = tff.federated_map(server_update_fn_h1, mean_client_weights)\n",
    "\n",
    "\n",
    "  return server_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695201990241,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "BPb71DfAwY4-"
   },
   "outputs": [],
   "source": [
    "federated_algorithm_h1 = tff.templates.IterativeProcess(\n",
    "    initialize_fn=initialize_fn_h1,\n",
    "    next_fn=next_fn_h1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCZHhE6gwY4-"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqN_PxJowY4_"
   },
   "source": [
    "### Creating test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201990242,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "F1t8y1yKwY4_"
   },
   "outputs": [],
   "source": [
    "def create_test_set(federated_test_set, num_classes):\n",
    "  # Create a list to store client datasets\n",
    "  client_datasets = []\n",
    "\n",
    "  # Iterate over client IDs and create datasets\n",
    "  for client_id in sorted(federated_test_set.client_ids)[:NUM_CLIENTS]:\n",
    "      client_dataset = federated_test_set.create_tf_dataset_for_client(client_id)\n",
    "      client_datasets.append(client_dataset)\n",
    "\n",
    "  # Combine the client datasets into a centralized dataset\n",
    "  test_set = tf.data.experimental.sample_from_datasets(client_datasets)\n",
    "  test_set = preprocess(test_set, num_classes)\n",
    "  return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1695201990538,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "ldxUgObPwY5A",
    "outputId": "6dfcfe3a-1003-4497-c5fc-d0a4a37845a9"
   },
   "outputs": [],
   "source": [
    "phase_test_central = create_test_set(phase_test_federated_data, PHASE_CLASSES)\n",
    "phase_train_central = create_test_set(phase_train_federated_data, PHASE_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1QxPIfDwY5B"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsoF-fa_FJjc"
   },
   "source": [
    "We conduct an initial evaluation to ensure the previous steps have been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695201990539,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "b2DU4chb0BJ8"
   },
   "outputs": [],
   "source": [
    "# Model compile instructions taken from Supervisor's code\n",
    "def evaluate_h1(server_state):\n",
    "  model = build_h1_model()\n",
    "  model.compile(\n",
    "      loss='categorical_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "  model.set_weights(server_state)\n",
    "  model.evaluate(phase_test_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 2661,
     "status": "ok",
     "timestamp": 1695201993197,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "sKW3LP3BwY5B"
   },
   "outputs": [],
   "source": [
    "server_state_h1 = federated_algorithm_h1.initialize()\n",
    "# evaluate_h1(server_state_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR31mZfawY5C"
   },
   "source": [
    "## Build Eval Models (TFF glitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPQXyvrlFRp6"
   },
   "source": [
    "Due to the way tensorflow federated works, we will not be able to create new models once the training has begun. For this reason, we create the evaluaiton model before the training. This also allows us to track the performance of the model through training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1695201993928,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "WhsnKEKBwY5C"
   },
   "outputs": [],
   "source": [
    "h1_eval = build_h1_model()\n",
    "h1_eval.compile(\n",
    "      loss='categorical_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy']\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWsWZXM0BSRL"
   },
   "source": [
    "## Time Logging for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695201993928,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "vx04jvTYBWoc"
   },
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return \"{:02d}:{:02d}:{:02d}\".format(int(hours), int(minutes), int(seconds))\n",
    "\n",
    "def calculate_time(round, start_time, times_taken, total_rounds):\n",
    "    # Record the end time for the current round\n",
    "    end_time = time.time()\n",
    "    # Calculate the time taken for the current round\n",
    "    time_taken = end_time - start_time\n",
    "    # Append the time taken to the list of times\n",
    "    times_taken.append(time_taken)\n",
    "\n",
    "    # Calculate the average time taken for the previous rounds\n",
    "    avg_time_taken = sum(times_taken) / len(times_taken)\n",
    "    # Calculate the estimated remaining time for the remaining rounds\n",
    "    remaining_rounds = total_rounds - (round + 1)\n",
    "    estimated_remaining_time = remaining_rounds * avg_time_taken\n",
    "\n",
    "    # Display the time taken for the current round and the estimated remaining time\n",
    "    print('Time taken for round {:2d}: {}'.format(round, format_time(time_taken)))\n",
    "    print('Estimated remaining time: {}'.format(format_time(estimated_remaining_time)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7TVwCdAwY5D"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orlzdHqxkgL_"
   },
   "source": [
    "### Early stopping check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695201993928,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "K27fTLC8kf2p"
   },
   "outputs": [],
   "source": [
    "def early_stop_check(accuracy, best_accuracy, epochs_without_improvement):\n",
    "  improvement = accuracy - best_accuracy\n",
    "  if improvement > MIN_IMPROVEMENT:\n",
    "      best_accuracy = accuracy\n",
    "      epochs_without_improvement = 0\n",
    "  else:\n",
    "      epochs_without_improvement += 1\n",
    "  # Stop training if no improvement for PATIENCE epochs\n",
    "  if epochs_without_improvement >= PATIENCE:\n",
    "      print(\"Early stopping: No improvement of at least {} for {} epochs.\".format(MIN_IMPROVEMENT, PATIENCE))\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6xTLwfNkj-G"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsXxMGWvwY5D",
    "outputId": "3e5c8f6b-5be3-4736-abbe-567eb120bc5e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "# Define the early stopping parameters\n",
    "PATIENCE = 10  # Number of epochs to wait for improvement\n",
    "MIN_IMPROVEMENT = 0.001  # Minimum improvement threshold (adjust as needed)\n",
    "best_accuracy = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Open the CSV file for writing\n",
    "csv_file_path = 'logs' + '/' + LOGS_DIR + '/' + MODEL_NAME + '_train_test' + '.csv'\n",
    "csv_columns = ['Round', 'Global_Train_Loss', 'Global_Train_Accuracy', 'Global_Test_Loss', 'Global_Test_Accuracy']  # Add more columns as needed\n",
    "\n",
    "for client_id in client_ids:\n",
    "    csv_columns.extend([f'{client_id}_Test_Loss', f'{client_id}_Test_Accuracy', f'{client_id}_Train_Loss', f'{client_id}_Train_Accuracy'])\n",
    "\n",
    "# Write the CSV header\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    times_taken = []\n",
    "    for round in range(1, EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Set the server weights to be the result of performing federated averaging\n",
    "        # on the client weights after one epoch of training.\n",
    "        server_state_h1 = federated_algorithm_h1.next(server_state_h1, phase_train)\n",
    "\n",
    "        # Conduct an evaluation of the training epoch\n",
    "        h1_eval.set_weights(server_state_h1)\n",
    "        global_train_loss, global_train_accuracy = h1_eval.evaluate(phase_train_central, verbose=0)\n",
    "        global_test_loss, global_test_accuracy = h1_eval.evaluate(phase_test_central, verbose=0)\n",
    "        print('Round {:2d}: \\t global_test_loss={:.4f}, global_test_accuracy={:.4f} \\n\\t\\tglobal_train_loss={:.4f}, global_train_accuracy={:.4f}'\n",
    "              .format(round, global_test_loss, global_test_accuracy, global_train_loss, global_train_accuracy))\n",
    "\n",
    "        # Early stopping check\n",
    "        stop = early_stop_check(global_test_accuracy, best_accuracy, epochs_without_improvement)\n",
    "\n",
    "        # Evaluate the model on a per-client basis\n",
    "        client_metrics = {}\n",
    "        for client_id in client_ids:\n",
    "            client_dataset = phase_test_federated_data.create_tf_dataset_for_client(client_id)\n",
    "            client_dataset = preprocess(client_dataset, PHASE_CLASSES)\n",
    "            test_loss, test_accuracy = h1_eval.evaluate(client_dataset, verbose=0)\n",
    "\n",
    "            client_dataset = phase_train_federated_data.create_tf_dataset_for_client(client_id)\n",
    "            client_dataset = preprocess(client_dataset, PHASE_CLASSES)\n",
    "            train_loss, train_accuracy = h1_eval.evaluate(client_dataset, verbose=0)\n",
    "\n",
    "            # Save the metrics for each client\n",
    "            client_metrics[f'{client_id}_Test_Loss'] = test_loss\n",
    "            client_metrics[f'{client_id}_Test_Accuracy'] = test_accuracy\n",
    "            client_metrics[f'{client_id}_Train_Loss'] = train_loss\n",
    "            client_metrics[f'{client_id}_Train_Accuracy'] = train_accuracy\n",
    "\n",
    "        # Save the metrics to CSV\n",
    "        csv_row=({\n",
    "            'Round': round,\n",
    "            'Global_Train_Loss': global_train_loss,\n",
    "            'Global_Train_Accuracy': global_train_accuracy,\n",
    "            'Global_Test_Loss': global_test_loss,\n",
    "            'Global_Test_Accuracy': global_test_accuracy,\n",
    "            **client_metrics\n",
    "        })\n",
    "\n",
    "        csv_writer.writerow(csv_row)\n",
    "\n",
    "        # Call the function to calculate time and display information\n",
    "        calculate_time(round, start_time, times_taken, EPOCHS+1)\n",
    "\n",
    "        # Stop if converged early\n",
    "        if stop:\n",
    "            print(\"Leaving training loop\")\n",
    "            round = EPOCHS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXMWOwCIwY5E"
   },
   "source": [
    "## Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i13vq5fPwY5E"
   },
   "outputs": [],
   "source": [
    "h1_eval.set_weights(server_state_h1)\n",
    "h1_eval.evaluate(phase_test_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 644,
     "status": "error",
     "timestamp": 1695223135235,
     "user": {
      "displayName": "Harrison Field",
      "userId": "02417387657116967688"
     },
     "user_tz": -60
    },
    "id": "1m36uRDbwY5F",
    "outputId": "0f21c03c-f873-4632-ce40-ad14e3e3596e"
   },
   "outputs": [],
   "source": [
    "h1_eval.save('Models/' + MODEL_NAME + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30MkkIcoLCp6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l6PhMlfRu6n"
   },
   "outputs": [],
   "source": [
    "# summary_writer_h1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s1FERdm84SY"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCoU9vyObIEH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "_kF4uur5CqEL",
    "j4mBmgkcClWX",
    "9FV6zs1AwY4Q",
    "EtyIuN51wY4f",
    "EiH95XycwY4p",
    "ZqmhVaMvwY4r",
    "sP1avAWNwY4u",
    "BCZHhE6gwY4-",
    "LqN_PxJowY4_",
    "-1QxPIfDwY5B",
    "nR31mZfawY5C",
    "sWsWZXM0BSRL",
    "orlzdHqxkgL_"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
